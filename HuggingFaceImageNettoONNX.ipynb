{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMT/mne1MX8WkhcJt4pXfLJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidAtRedpine/HuggingFaceImageNettoONNX/blob/main/HuggingFaceImageNettoONNX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace ImageNet to ONNX\n",
        "\n",
        "Convert HuggingFace model trained on ImageNet into the ONNX format"
      ],
      "metadata": {
        "id": "tp09P60G-AlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, import the necessary Python modules"
      ],
      "metadata": {
        "id": "xu0R2-RCB7lU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "! pip install onnx onnxruntime"
      ],
      "metadata": {
        "id": "NlWwRhtFNFBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQdJRdfF9zpg"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import AutoModelForImageClassification\n",
        "from PIL import Image\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "from google.colab import files\n",
        "import requests\n",
        "from io import BytesIO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define your variables\n",
        "\n",
        "Put the path to your HuggingFace model that was trained on ImageNet. In this example, we are using [resnet-50](https://huggingface.co/microsoft/resnet-50) . Also set the width/height of the images (default 224px), and provide a URL to a sample image to test that the ONNX model works.\n"
      ],
      "metadata": {
        "id": "ZX5PF1IKCNGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model_path = \"microsoft/resnet-50\" #@param {type:\"string\"}\n",
        "resolution = 224 #@param {type:\"integer\"}\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f7/003_Olive-bellied_Sunbird_in_flight_at_Kibale_forest_National_Park_Photo_by_Giles_Laurent.jpg/1280px-003_Olive-bellied_Sunbird_in_flight_at_Kibale_forest_National_Park_Photo_by_Giles_Laurent.jpg\" #@param {type:\"string\"}\n",
        "export_model_name = \"model.onnx\" #@param {type:\"string\"}\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BeCkzR-3CbwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert the model to ONNX"
      ],
      "metadata": {
        "id": "3LLlTWL4Ml1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = AutoModelForImageClassification.from_pretrained(pretrained_model_path)\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "# Get the list of labels from the model's configuration\n",
        "labels = list(model.config.id2label.values())\n",
        "\n",
        "# Define the preprocessing transformation\n",
        "# mean=[0.485, 0.456, 0.406]: These are the average values of the\n",
        "# pixel intensities for the red, green, and blue channels, respectively.\n",
        "# std=[0.229, 0.224, 0.225]: These are the standard deviations of the\n",
        "# pixel intensities for the red, green, and blue channels, respectively.\n",
        "# These values are typically used with ImageNet dataset models.\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((resolution, resolution)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create a dummy input Tensor for exporting\n",
        "dummy_input = torch.randn(1, 3, resolution, resolution)\n",
        "\n",
        "# Export the model to ONNX format\n",
        "input_names = [\"input\"]\n",
        "output_names = [\"output\"]\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    export_model_name,\n",
        "    export_params=True,\n",
        "    opset_version=11,\n",
        "    do_constant_folding=True,\n",
        "    input_names=input_names,\n",
        "    output_names=output_names,\n",
        "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
        ")\n"
      ],
      "metadata": {
        "id": "D8eF3gD6Mo7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validate the ONNX model works"
      ],
      "metadata": {
        "id": "EgPph6AJQ242"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Validate the ONNX model\n",
        "onnx_model = onnx.load(export_model_name)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "# Create an ONNX Runtime session\n",
        "ort_session = ort.InferenceSession(export_model_name)\n",
        "\n",
        "# Prepare a sample input image\n",
        "image = load_image_from_url(image_url)\n",
        "input_tensor = preprocess(image.convert(\"RGB\")).unsqueeze(0)\n",
        "\n",
        "# Run the ONNX model\n",
        "ort_inputs = {input_names[0]: input_tensor.numpy()}\n",
        "ort_outputs = ort_session.run(None, ort_inputs)\n",
        "ort_output_logits = ort_outputs[0]\n",
        "\n",
        "# Get the predicted label index\n",
        "predicted_idx = ort_output_logits.argmax(axis=1)[0]\n",
        "\n",
        "# Get the predicted label text\n",
        "predicted_label = labels[predicted_idx]\n",
        "\n",
        "# Print the predicted label\n",
        "print(\"Detected label:\", predicted_label)"
      ],
      "metadata": {
        "id": "XspaszkSQ5jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the ONNX file"
      ],
      "metadata": {
        "id": "VIL_ky8QPpzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(export_model_name)"
      ],
      "metadata": {
        "id": "yCfjaPeXPrae"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}